{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 5\n",
    "\n",
    "This is Module 5 in building a reusable, class-based machine learning framework.\n",
    "\n",
    "In [Module 1](https://github.com/threnjen/object_oriented_machine_learning/blob/main/module1/module_1.ipynb), we focused on preparing to code outside of Jupyter Notebook. We set up virtual environments with [Pipenv](https://pipenv.pypa.io/en/latest/), installed and configured [VS Code](https://code.visualstudio.com/), and added automatic [Black](https://github.com/psf/black) formatting to our code.\n",
    "\n",
    "In [Module 2](https://github.com/threnjen/object_oriented_machine_learning/blob/main/module2/module_2.ipynb), we refreshed about how to build Python classes. We built our base model class with an initial exploratory method. We added docstrings to our class methods so that we get documentation when calling help().\n",
    "\n",
    "In [Module 3](https://github.com/threnjen/object_oriented_machine_learning/blob/main/module3/module_3.ipynb), we built an EDA Cleaning class and integrated it into our Base Model.\n",
    "\n",
    "In [Module 4](https://github.com/threnjen/object_oriented_machine_learning/blob/main/module4/module_4.ipynb), we introduced the concept of an abstract class and started our Regression abstract class, as well as introduced type hints into our code.\n",
    "\n",
    "In Module 5, we will largely focus on extending our Cleaning class and our Regression class, and use/add the following elements:\n",
    "- Default parameter values\n",
    "- Add an undo function that can undo our last alteration method (maybe you dropped outliers by IQR, then change your mind)\n",
    "- Additional EDA methods\n",
    "- Additional Cleaning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look back at our BaseModel and EDACleaning objects, as we left them in Module 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Literal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class BaseModel:\n",
    "    def __init__(self, filename: str):\n",
    "        self.df = self._load_file(filename)\n",
    "        self.target = None\n",
    "        self.cleaner = EDACleaning()\n",
    "\n",
    "    def _load_file(self, filename: str) -> pd.DataFrame:\n",
    "        \"\"\"Load file from filename and set target field\n",
    "        Args:\n",
    "            filename (str): filename in csv format\n",
    "        Returns:\n",
    "            pd.DataFrame: df loaded from file\n",
    "        \"\"\"\n",
    "        return pd.read_csv(filename, on_bad_lines=\"skip\")\n",
    "\n",
    "    def set_target(self, target: str) -> None:\n",
    "        \"\"\"Sets model target field\n",
    "        Args:\n",
    "            target (str): target: target field for model\n",
    "        \"\"\"\n",
    "        self.target = target\n",
    "        self.cleaner.set_target(target)\n",
    "\n",
    "    def print_statistics(self) -> None:\n",
    "        \"\"\"Print basic statistics for data\"\"\"\n",
    "        self.cleaner.print_statistics(self.df)\n",
    "\n",
    "    @abstractmethod\n",
    "    def split_data(self, stratify: Optional[bool]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def basic_regression(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDACleaning:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_target(self, target: str) -> None:\n",
    "        \"\"\"set model target variable\"\"\"\n",
    "        self.target = target\n",
    "\n",
    "    def print_statistics(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Print basic dataframe statistics\"\"\"\n",
    "        print(df.head())\n",
    "        print(f\"DF shape: {df.shape}\\n\")\n",
    "        print(f\"Data types: {df.dtypes}\\n\")\n",
    "        print(f\"Describe: {df.describe()}\\n\")\n",
    "        print(f\"isna sum: {df.isna().sum()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first plan is to add some new EDA methods to our EDACleaning class. The EDA methods are easy to identify because they do not return anything. Any of our modification/cleaning methods found in our EDACleaning class will return an altered data frame.\n",
    "\n",
    "Our new methods are:\n",
    "- print_sorted\n",
    "- check_value_counts\n",
    "- find_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDACleaning:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_target(self, target: str):\n",
    "        \"\"\"set model target variable\"\"\"\n",
    "        self.target = target\n",
    "\n",
    "    def print_statistics(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Print basic dataframe statistics\"\"\"\n",
    "        print(df.head())\n",
    "        print(f\"DF shape: {df.shape}\\n\")\n",
    "        print(f\"Data types: {df.dtypes}\\n\")\n",
    "        print(f\"Describe: {df.describe()}\\n\")\n",
    "        print(f\"isna sum: {df.isna().sum()}\\n\")\n",
    "\n",
    "    def print_sorted(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        field: str,\n",
    "        groupby: Optional[str],\n",
    "        asc: bool,\n",
    "    ) -> None:\n",
    "        if groupby:\n",
    "            print(df.groupby(groupby)[field].mean().sort_values(ascending=asc))\n",
    "        else:\n",
    "            print(df.sort_values(field, ascending=asc).head())\n",
    "\n",
    "    def check_value_counts(self, df: pd.DataFrame, field: str) -> None:\n",
    "        print(df[field].value_counts(normalize=True).head())\n",
    "\n",
    "    def find_outliers(self, field: str) -> None:\n",
    "        find_outliers = self.df.groupby(field)[self.target].describe()\n",
    "        find_outliers.sort_values(\"mean\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add methods into our BaseModel class that call on these.\n",
    "\n",
    "We are using our BaseModel class as our dispatcher for everything that we do, so that we, as eventual users of the system, need to know as little as necessary to use our work. If we don't use dispatch methods from within our BaseModel, that means we would need to call `help()` separately on our different modules, and figure out where a particular method lives. Then to call the method we would need to call our self.other_module from our model, making for long and messy method calls, such as `model.cleaner.print_sorted(df=my_df)`. Instead, we can just call help on our model object, and get all of the most minimal information required in order to use the system. For that same complicated example call just presented, instead we will only need to call `model.print_sorted()` to accomplish exactly the same thing.\n",
    "\n",
    "Generally, these \"pass-through\" methods that simply call a method in another class are discouraged. In standard software design you'd want to avoid them. We appreciate them in our case because our end use of this system will be manually within Jupyter Notebook, and we're making our system as simple for ourselves to use as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    def __init__(self, filename: str, seed: Optional[int] = None):\n",
    "        self.df = self._load_file(filename)\n",
    "        self.target = None\n",
    "        self.cleaner = EDACleaning()\n",
    "\n",
    "    def _load_file(self, filename: str) -> pd.DataFrame:\n",
    "        \"\"\"Load file from filename and set target field\n",
    "        Args:\n",
    "            filename (str): filename in csv format\n",
    "        Returns:\n",
    "            pd.DataFrame: df loaded from file\n",
    "        \"\"\"\n",
    "        return pd.read_csv(filename, on_bad_lines=\"skip\")\n",
    "\n",
    "    def set_target(self, target: str) -> None:\n",
    "        \"\"\"Sets model target field\n",
    "        Args:\n",
    "            target (str): target: target field for model\n",
    "        \"\"\"\n",
    "        self.target = target\n",
    "        self.cleaner.set_target(target)\n",
    "\n",
    "    def print_statistics(self) -> None:\n",
    "        \"\"\"Print basic statistics for data\"\"\"\n",
    "        self.cleaner.print_statistics(self.df)\n",
    "\n",
    "    def print_sorted(\n",
    "        self,\n",
    "        field: Optional[str] = None,\n",
    "        groupby: Optional[str] = None,\n",
    "        asc: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Prints sorted based on provided field. Will use target if no field provided.\n",
    "        Args:\n",
    "            field (_type_, optional): Will sort by this field. Defaults to target.\n",
    "            asc (bool, optional): Sort ascending. Defaults to False.\n",
    "            groupby (_type_, optional): If entered, will group by this field. Defaults to None.\n",
    "        \"\"\"\n",
    "        if not field:\n",
    "            field = self.target\n",
    "        self.cleaner.print_sorted(df=self.df, field=field, asc=asc, groupby=groupby)\n",
    "\n",
    "    def check_value_counts(self, field: Optional[str] = None) -> None:\n",
    "        \"\"\"Will print value counts for field\n",
    "        Args:\n",
    "            field (_type_, optional): Will report on this field. Defaults to target.\n",
    "        \"\"\"\n",
    "        if not field:\n",
    "            field = self.target\n",
    "        self.cleaner.check_value_counts(self.df, field)\n",
    "\n",
    "    def find_outliers(self, field: str) -> None:\n",
    "        self.cleaner.find_outliers(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that in our new methods, print_sorted in particular, we are making use of default arguments. These are the parameters that the function will use automatically if not given any alternative arguments. Default behavior is a powerful tool to reduce the complexity of our user interface.\n",
    "\n",
    "The print_sorted method can take several arguments - a field to sort on, a groupby field to group with first, and a switch to ascending=True. However the method doesn't require any of these, and in fact will be callable with a plain `model.print_sorted()` and automatically sort on the target field. We can optionally add other arguments to the method when we call it if we want different information, such as `model.print_sorted(groupby='bedrooms', asc=True)`\n",
    "\n",
    "Remember we can always use the handy help() call to find out what our options are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BaseModel in module __main__:\n",
      "\n",
      "class BaseModel(abc.ABC)\n",
      " |  BaseModel(filename: str, seed: Optional[int] = None)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BaseModel\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, filename: str, seed: Optional[int] = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  check_value_counts(self, field=None) -> None\n",
      " |      Will print value counts for field\n",
      " |      Args:\n",
      " |          field (_type_, optional): Will report on this field. Defaults to target.\n",
      " |  \n",
      " |  find_outliers(self, field: str) -> None\n",
      " |  \n",
      " |  print_sorted(self, field: Optional[str] = None, groupby: Optional[str] = None, asc: Optional[bool] = False) -> None\n",
      " |      Prints sorted based on provided field. Will use target if no field provided.\n",
      " |      Args:\n",
      " |          field (_type_, optional): Will sort by this field. Defaults to target.\n",
      " |          asc (bool, optional): Sort ascending. Defaults to False.\n",
      " |          groupby (_type_, optional): If entered, will group by this field. Defaults to None.\n",
      " |  \n",
      " |  print_statistics(self) -> None\n",
      " |      Print basic statistics for data\n",
      " |  \n",
      " |  set_target(self, target: str) -> None\n",
      " |      Sets model target field\n",
      " |      Args:\n",
      " |          target (str): target: target field for model\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(BaseModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to add some methods to our EDACleaning object that actually alter our original dataframe! The new methods are added at the bottom:\n",
    "- drop_dupes\n",
    "- remove_outliers\n",
    "- _calculate_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDACleaning:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_target(self, target: str):\n",
    "        \"\"\"set model target variable\"\"\"\n",
    "        self.target = target\n",
    "\n",
    "    def print_statistics(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Print basic dataframe statistics\"\"\"\n",
    "        print(df.head())\n",
    "        print(f\"DF shape: {df.shape}\\n\")\n",
    "        print(f\"Data types: {df.dtypes}\\n\")\n",
    "        print(f\"Describe: {df.describe()}\\n\")\n",
    "        print(f\"isna sum: {df.isna().sum()}\\n\")\n",
    "\n",
    "    def print_sorted(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        field: str,\n",
    "        groupby: Optional[str],\n",
    "        asc: bool,\n",
    "    ) -> None:\n",
    "        if groupby:\n",
    "            print(df.groupby(groupby)[field].mean().sort_values(ascending=asc))\n",
    "        else:\n",
    "            print(df.sort_values(field, ascending=asc).head())\n",
    "\n",
    "    def check_value_counts(self, df: pd.DataFrame, field: str) -> None:\n",
    "        print(df[field].value_counts(normalize=True).head())\n",
    "\n",
    "    def find_outliers(self, field: str) -> None:\n",
    "        find_outliers = self.df.groupby(field)[self.target].describe()\n",
    "        find_outliers.sort_values(\"mean\", ascending=False).head(20)\n",
    "\n",
    "    def drop_dupes(self, df: pd.DataFrame, subset: list=None) -> pd.DataFrame:\n",
    "        if subset:\n",
    "            df.drop_duplicates(subset, keep=\"last\", inplace=True)\n",
    "        else:\n",
    "            df.drop_duplicates(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def remove_outliers(\n",
    "        self, df: pd.DataFrame, fields: list, method: str, range: float\n",
    "    ) -> pd.DataFrame:\n",
    "        if method == \"iqr\":\n",
    "            for field in fields:\n",
    "                lower_range, upper_range = self._calculate_iqr(df[field], range)\n",
    "                df = df.drop(\n",
    "                    df[(df[field] > upper_range) | (df[field] < lower_range)].index\n",
    "                )\n",
    "        return df\n",
    "\n",
    "    def _calculate_iqr(self, column: pd.Series, range: float) -> Literal(float, float):\n",
    "        \"\"\"return the lower range and upper range for the data based on IQR\n",
    "        Arguments:\n",
    "        column - column to be evaluated\n",
    "        iqr_level - iqr range to be evaluated\n",
    "        \"\"\"\n",
    "        Q1, Q3 = np.percentile(column, [25, 75])\n",
    "        iqr = Q3 - Q1\n",
    "        lower_range = Q1 - (range * iqr)\n",
    "        upper_range = Q3 + (range * iqr)\n",
    "        return lower_range, upper_range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll might notice that in our type hints, these functions note that they return a pd.DataFrame instead of None. These are our first cleaning functions that actually make a modification to our original dataframe.\n",
    "\n",
    "Now we add the methods that call these to our BaseModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    def __init__(self, filename: str, seed: Optional[int] = None):\n",
    "        self.df = self._load_file(filename)\n",
    "        self.target = None\n",
    "        self.cleaner = EDACleaning()\n",
    "\n",
    "    def _load_file(self, filename: str) -> pd.DataFrame:\n",
    "        \"\"\"Load file from filename and set target field\n",
    "        Args:\n",
    "            filename (str): filename in csv format\n",
    "        Returns:\n",
    "            pd.DataFrame: df loaded from file\n",
    "        \"\"\"\n",
    "        return pd.read_csv(filename, on_bad_lines=\"skip\")\n",
    "\n",
    "    def set_target(self, target: str) -> None:\n",
    "        \"\"\"Sets model target field\n",
    "        Args:\n",
    "            target (str): target: target field for model\n",
    "        \"\"\"\n",
    "        self.target = target\n",
    "        self.cleaner.set_target(target)\n",
    "\n",
    "    def print_statistics(self) -> None:\n",
    "        \"\"\"Print basic statistics for data\"\"\"\n",
    "        self.cleaner.print_statistics(self.df)\n",
    "\n",
    "    def print_sorted(\n",
    "        self,\n",
    "        field: Optional[str] = None,\n",
    "        groupby: Optional[str] = None,\n",
    "        asc: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Prints sorted based on provided field. Will use target if no field provided.\n",
    "        Args:\n",
    "            field (_type_, optional): Will sort by this field. Defaults to target.\n",
    "            asc (bool, optional): Sort ascending. Defaults to False.\n",
    "            groupby (_type_, optional): If entered, will group by this field. Defaults to None.\n",
    "        \"\"\"\n",
    "        if not field:\n",
    "            field = self.target\n",
    "        self.cleaner.print_sorted(df=self.df, field=field, asc=asc, groupby=groupby)\n",
    "\n",
    "    def check_value_counts(self, field: Optional[str] = None) -> None:\n",
    "        \"\"\"Will print value counts for field\n",
    "        Args:\n",
    "            field (_type_, optional): Will report on this field. Defaults to target.\n",
    "        \"\"\"\n",
    "        if not field:\n",
    "            field = self.target\n",
    "        self.cleaner.check_value_counts(self.df, field)\n",
    "\n",
    "    def find_outliers(self, field: str) -> None:\n",
    "        self.cleaner.find_outliers(field)\n",
    "\n",
    "    def drop_dupes(self, subset: Optional[list] = None):\n",
    "        \"\"\"drops duplicate dataframe rows\n",
    "        Args:\n",
    "            subset (list, optional): Subset on which to drop dupes. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.df = self.cleaner.drop_dupes(self.df, subset)\n",
    "\n",
    "    def remove_outliers(\n",
    "        self,\n",
    "        fields: list = [],\n",
    "        method: Optional[str] = \"iqr\",\n",
    "        range: Optional[float] = 1.5,\n",
    "        save: Optional[bool] = True,\n",
    "    ) -> None:\n",
    "        \"\"\"removes outliers, defaultings to IQR with a default IQR range of 1.5\n",
    "        Args:\n",
    "            fields (list): list of fields. Must be list even if one item.\n",
    "            method (str, optional): outlier removal method. Defaults to \"iqr\".\n",
    "            range (float, optional): IQR range. Defaults to 1.5.\n",
    "        \"\"\"\n",
    "        self.df = self.cleaner.remove_outliers(self.df, fields, method, range)\n",
    "\n",
    "    def reset_df_index(self, save: Optional[bool] = True) -> None:\n",
    "        \"\"\"resets dataframe index\"\"\"\n",
    "        print(self.df.head())\n",
    "        self.df.reset_index(inplace=True, drop=True)\n",
    "        print(self.df.head())\n",
    "\n",
    "    @abstractmethod\n",
    "    def split_data(self, stratify: Optional[bool]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def basic_regression(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice a method here that wasn't in the EDACleaning object. reset_df_index is only located here. This method is SO simple that I opted to keep its full implementation here instead of using a pass-through method - but for the sake of consistency, you could opt to pass it through.\n",
    "\n",
    "We could be done here, but now that we've added methods that change our initial data, we've run into a potential problem point. What if we make a change and then don't like our change? As it stands, we would have to go all the way back to the beginning of our work and remake our model object from scratch with our filename, in order to reset the data.\n",
    "\n",
    "Instead of doing that, we're going to implement an undo() method that undoes our last change call. We're going to do the following steps:\n",
    "- Implement a _set_save() method that saves our dataframe state\n",
    "- Implement an undo() method that restores the last saved state\n",
    "- Insert a save() call into any method that makes data frame changes, and default saving to True.\n",
    "\n",
    "Here's what this all looks like, implemented in our BaseModel class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    def __init__(self, filename: str, seed: Optional[int] = None):\n",
    "        self.df = self._load_file(filename)\n",
    "        self.target = None\n",
    "        self.cleaner = EDACleaning()\n",
    "\n",
    "    def _load_file(self, filename: str) -> pd.DataFrame:\n",
    "        \"\"\"Load file from filename and set target field\n",
    "        Args:\n",
    "            filename (str): filename in csv format\n",
    "        Returns:\n",
    "            pd.DataFrame: df loaded from file\n",
    "        \"\"\"\n",
    "        return pd.read_csv(filename, on_bad_lines=\"skip\")\n",
    "\n",
    "    def set_target(self, target: str) -> None:\n",
    "        \"\"\"Sets model target field\n",
    "        Args:\n",
    "            target (str): target: target field for model\n",
    "        \"\"\"\n",
    "        self.target = target\n",
    "        self.cleaner.set_target(target)\n",
    "\n",
    "    def _set_save(self, saved: str) -> None:\n",
    "        \"\"\"Sets a save point on the dataframe before performing an alteration task\n",
    "\n",
    "        Args:\n",
    "            saved (str): description of saved task to report if undone\n",
    "        \"\"\"\n",
    "        self.saved_df = self.df.copy()\n",
    "        self.saved_action = saved\n",
    "\n",
    "    def undo(self) -> None:\n",
    "        \"\"\"Undoes the last data frame alteration task, and reports on Undo\"\"\"\n",
    "        self.df = self.saved_df()\n",
    "        print(f\"Undid last change: {self.saved_action}\")\n",
    "\n",
    "    def print_statistics(self) -> None:\n",
    "        \"\"\"Print basic statistics for data\"\"\"\n",
    "        self.cleaner.print_statistics(self.df)\n",
    "\n",
    "    def print_sorted(\n",
    "        self,\n",
    "        field: Optional[str] = None,\n",
    "        groupby: Optional[str] = None,\n",
    "        asc: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Prints sorted based on provided field. Will use target if no field provided.\n",
    "        Args:\n",
    "            field (_type_, optional): Will sort by this field. Defaults to target.\n",
    "            asc (bool, optional): Sort ascending. Defaults to False.\n",
    "            groupby (_type_, optional): If entered, will group by this field. Defaults to None.\n",
    "        \"\"\"\n",
    "        if not field:\n",
    "            field = self.target\n",
    "        self.cleaner.print_sorted(df=self.df, field=field, asc=asc, groupby=groupby)\n",
    "\n",
    "    def check_value_counts(self, field: Optional[str] = None) -> None:\n",
    "        \"\"\"Will print value counts for field\n",
    "        Args:\n",
    "            field (_type_, optional): Will report on this field. Defaults to target.\n",
    "        \"\"\"\n",
    "        if not field:\n",
    "            field = self.target\n",
    "        self.cleaner.check_value_counts(self.df, field)\n",
    "\n",
    "    def find_outliers(self, field: str) -> None:\n",
    "        self.cleaner.find_outliers(field)\n",
    "\n",
    "    def drop_dupes(self, subset: Optional[list] = None, save: Optional[bool] = True):\n",
    "        \"\"\"Save point, then drops duplicate dataframe rows\n",
    "        Args:\n",
    "            subset (list, optional): Subset on which to drop dupes. Defaults to None.\n",
    "            save (boolean, optional): Toggles to save. Defaults to None.\n",
    "        \"\"\"\n",
    "        if save:\n",
    "            self._set_save(\"drop_dupes\")\n",
    "        self.df = self.cleaner.drop_dupes(self.df, subset)\n",
    "\n",
    "    def remove_outliers(\n",
    "        self,\n",
    "        fields: list = [],\n",
    "        method: Optional[str] = \"iqr\",\n",
    "        range: Optional[float] = 1.5,\n",
    "        save: Optional[bool] = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Save point, then removes outliers, defaultings to IQR with a default IQR range of 1.5\n",
    "        Args:\n",
    "            fields (list): list of fields. Must be list even if one item.\n",
    "            method (str, optional): outlier removal method. Defaults to \"iqr\".\n",
    "            range (float, optional): IQR range. Defaults to 1.5.\n",
    "            save (boolean, optional): Toggles to save. Defaults to None.\n",
    "        \"\"\"\n",
    "        if save:\n",
    "            self._set_save(\"remove_outliers\")\n",
    "        self.df = self.cleaner.remove_outliers(self.df, fields, method, range)\n",
    "\n",
    "    def reset_index(self, save: Optional[bool] = True) -> None:\n",
    "        \"\"\"Save point, then resets dataframe index\"\"\"\n",
    "        print(self.df.head())\n",
    "        if save:\n",
    "            self._set_save(\"reset_index\")\n",
    "        self.df.reset_index(inplace=True, drop=True)\n",
    "        print(self.df.head())\n",
    "\n",
    "    @abstractmethod\n",
    "    def split_data(self, stratify: Optional[bool]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def basic_regression(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('object_oriented_machine_learning-cyZdX5gt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c92e476e37dcf0c11c9fec65999947cee5b5777dc27b66638545f02a2906077b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
