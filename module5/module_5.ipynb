{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 5\n",
    "\n",
    "This is Module 5 in building a reusable, class-based machine learning framework.\n",
    "\n",
    "In [Module 1](https://github.com/threnjen/object_oriented_machine_learning/blob/main/module1/module_1.ipynb), we focused on preparing to code outside of Jupyter Notebook. We set up virtual environments with [Pipenv](https://pipenv.pypa.io/en/latest/), installed and configured [VS Code](https://code.visualstudio.com/), and added automatic [Black](https://github.com/psf/black) formatting to our code.\n",
    "\n",
    "In [Module 2](https://github.com/threnjen/object_oriented_machine_learning/blob/main/module2/module_2.ipynb), we refreshed about how to build Python classes. We built our base model class with an initial exploratory method. We added docstrings to our class methods so that we get documentation when calling help().\n",
    "\n",
    "In [Module 3](https://github.com/threnjen/object_oriented_machine_learning/blob/main/module3/module_3.ipynb), we built an EDA Cleaning class and integrated it into our Base Model.\n",
    "\n",
    "In [Module 4](https://github.com/threnjen/object_oriented_machine_learning/blob/main/module4/module_4.ipynb), we introduced the concept of an abstract class and started our Regression abstract class, as well as introduced type hints into our code.\n",
    "\n",
    "In Module 5, we will largely focus on extending our Cleaning class, and use/add the following elements:\n",
    "- Default parameter values\n",
    "- Add an undo function that can undo our last alteration method (maybe you dropped outliers by IQR, then change your mind)\n",
    "- Additional EDA methods\n",
    "- Additional Cleaning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look back at our BaseModel and EDACleaning objects, as we left them in Module 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Literal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class BaseModel:\n",
    "    def __init__(self, filename: str):\n",
    "        self.df = self._load_file(filename)\n",
    "        self.target = None\n",
    "        self.cleaner = EDACleaning()\n",
    "\n",
    "    def _load_file(self, filename: str) -> pd.DataFrame:\n",
    "        \"\"\"Load file from filename and set target field\n",
    "        Args:\n",
    "            filename (str): filename in csv format\n",
    "        Returns:\n",
    "            pd.DataFrame: df loaded from file\n",
    "        \"\"\"\n",
    "        return pd.read_csv(filename, on_bad_lines=\"skip\")\n",
    "\n",
    "    def set_target(self, target: str) -> None:\n",
    "        \"\"\"Sets model target field\n",
    "        Args:\n",
    "            target (str): target: target field for model\n",
    "        \"\"\"\n",
    "        self.target = target\n",
    "        self.cleaner.set_target(target)\n",
    "\n",
    "    def print_statistics(self) -> None:\n",
    "        \"\"\"Print basic statistics for data\"\"\"\n",
    "        self.cleaner.print_statistics(self.df)\n",
    "\n",
    "    @abstractmethod\n",
    "    def split_data(self, stratify: Optional[bool]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def basic_regression(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDACleaning:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_target(self, target: str) -> None:\n",
    "        \"\"\"set model target variable\"\"\"\n",
    "        self.target = target\n",
    "\n",
    "    def print_statistics(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Print basic dataframe statistics\"\"\"\n",
    "        print(df.head())\n",
    "        print(f\"DF shape: {df.shape}\\n\")\n",
    "        print(f\"Data types: {df.dtypes}\\n\")\n",
    "        print(f\"Describe: {df.describe()}\\n\")\n",
    "        print(f\"isna sum: {df.isna().sum()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first plan is to add some new EDA methods to our EDACleaning class. The EDA methods are easy to identify because they do not return anything. The cleaning methods that we put into the EDACleaning class will return an altered data frame.\n",
    "\n",
    "Our new methods are:\n",
    "- print_sorted\n",
    "- check_value_proportions\n",
    "- find_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDACleaning:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_target(self, target: str):\n",
    "        \"\"\"set model target variable\"\"\"\n",
    "        self.target = target\n",
    "\n",
    "    def print_statistics(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Print basic dataframe statistics\"\"\"\n",
    "        print(df.head())\n",
    "        print(f\"DF shape: {df.shape}\\n\")\n",
    "        print(f\"Data types: {df.dtypes}\\n\")\n",
    "        print(f\"Describe: {df.describe()}\\n\")\n",
    "        print(f\"isna sum: {df.isna().sum()}\\n\")\n",
    "\n",
    "    def print_sorted(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        field: str,\n",
    "        groupby: Optional[str],\n",
    "        asc: bool,\n",
    "    ) -> None:\n",
    "        if groupby:\n",
    "            print(df.groupby(groupby)[field].mean().sort_values(ascending=asc))\n",
    "        else:\n",
    "            print(df.sort_values(field, ascending=asc).head())\n",
    "\n",
    "    def check_value_proportions(self, df: pd.DataFrame, field: str) -> None:\n",
    "        print(df[field].value_counts(normalize=True).head())\n",
    "\n",
    "    def find_outliers(self, df: pd.DataFrame, field: str) -> None:\n",
    "        find_outliers = df.groupby(field)[self.target].describe()\n",
    "        outlier_report = find_outliers.sort_values(\"mean\", ascending=False).head(20)\n",
    "        print(outlier_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add methods into our BaseModel class that call on these.\n",
    "\n",
    "We are using our BaseModel class as our dispatcher for everything that we do, so that we, as eventual users of the system, need to know as little as necessary to use our work. If we don't use dispatch methods from within our BaseModel, that means we would need to call `help()` separately on our different modules, and figure out where a particular method lives. Then to call the method we would need to call our self.other_module from our model, making for long and messy method calls, such as `model.cleaner.print_sorted(df=my_df)`. Instead, we can just call help on our model object, and get all of the most minimal information required in order to use the system. For that same complicated example call just presented, instead we will only need to call `model.print_sorted()` to accomplish exactly the same thing.\n",
    "\n",
    "Generally, \"pass-through\" methods that simply call a method in another class are discouraged. In standard software design you'd want to avoid them. We appreciate them in our use case, because our end use of this system will be done manually from within Jupyter Notebook. We're making our system as simple for ourselves to use as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    def __init__(self, filename: str):\n",
    "        self.df = self._load_file(filename)\n",
    "        self.target = None\n",
    "        self.cleaner = EDACleaning()\n",
    "\n",
    "    def _load_file(self, filename: str) -> pd.DataFrame:\n",
    "        \"\"\"Load file from filename and set target field\n",
    "        Args:\n",
    "            filename (str): filename in csv format\n",
    "        Returns:\n",
    "            pd.DataFrame: df loaded from file\n",
    "        \"\"\"\n",
    "        return pd.read_csv(filename, on_bad_lines=\"skip\")\n",
    "\n",
    "    def set_target(self, target: str) -> None:\n",
    "        \"\"\"Sets model target field\n",
    "        Args:\n",
    "            target (str): target: target field for model\n",
    "        \"\"\"\n",
    "        self.target = target\n",
    "        self.cleaner.set_target(target)\n",
    "\n",
    "    def print_statistics(self) -> None:\n",
    "        \"\"\"Print basic statistics for data\"\"\"\n",
    "        self.cleaner.print_statistics(self.df)\n",
    "\n",
    "    def print_sorted(\n",
    "        self,\n",
    "        field: Optional[str] = None,\n",
    "        groupby: Optional[str] = None,\n",
    "        asc: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Prints sorted based on provided field. Will use target if no field provided.\n",
    "        Args:\n",
    "            field (_type_, optional): Will sort by this field. Defaults to target.\n",
    "            asc (bool, optional): Sort ascending. Defaults to False.\n",
    "            groupby (_type_, optional): If entered, will group by this field. Defaults to None.\n",
    "        \"\"\"\n",
    "        if not field:\n",
    "            field = self.target\n",
    "        self.cleaner.print_sorted(df=self.df, field=field, asc=asc, groupby=groupby)\n",
    "\n",
    "    def check_value_proportions(self, field: Optional[str] = None) -> None:\n",
    "        \"\"\"Will print value counts for field\n",
    "        Args:\n",
    "            field (_type_, optional): Will report on this field. Defaults to target.\n",
    "        \"\"\"\n",
    "        if not field:\n",
    "            field = self.target\n",
    "        self.cleaner.check_value_proportions(self.df, field)\n",
    "\n",
    "    def find_outliers(self, field: str) -> None:\n",
    "        self.cleaner.find_outliers(self.df, field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that in our new methods, print_sorted in particular, we are making use of default arguments. These are the parameters that the function will use automatically if not given any alternative arguments. Default behavior is a powerful tool to reduce the complexity of our user interface.\n",
    "\n",
    "The print_sorted method can take several arguments - a field to sort on, a groupby field to group with first, and a switch to ascending=True. However the method doesn't require any of these, and in fact will be callable with a plain `model.print_sorted()` and automatically sort on the target field. We can optionally add other arguments to the method when we call it if we want different information, such as `model.print_sorted(groupby='bedrooms', asc=True)`\n",
    "\n",
    "Remember we can always use the handy help() call to find out what our options are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BaseModel in module __main__:\n",
      "\n",
      "class BaseModel(abc.ABC)\n",
      " |  BaseModel(filename: str)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BaseModel\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, filename: str)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  check_value_proportions(self, field: Optional[str] = None) -> None\n",
      " |      Will print value counts for field\n",
      " |      Args:\n",
      " |          field (_type_, optional): Will report on this field. Defaults to target.\n",
      " |  \n",
      " |  find_outliers(self, field: str) -> None\n",
      " |  \n",
      " |  print_sorted(self, field: Optional[str] = None, groupby: Optional[str] = None, asc: Optional[bool] = False) -> None\n",
      " |      Prints sorted based on provided field. Will use target if no field provided.\n",
      " |      Args:\n",
      " |          field (_type_, optional): Will sort by this field. Defaults to target.\n",
      " |          asc (bool, optional): Sort ascending. Defaults to False.\n",
      " |          groupby (_type_, optional): If entered, will group by this field. Defaults to None.\n",
      " |  \n",
      " |  print_statistics(self) -> None\n",
      " |      Print basic statistics for data\n",
      " |  \n",
      " |  set_target(self, target: str) -> None\n",
      " |      Sets model target field\n",
      " |      Args:\n",
      " |          target (str): target: target field for model\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(BaseModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using some of our EDA methods to look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
      "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
      "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
      "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
      "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
      "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
      "\n",
      "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
      "0      5650     1.0           0     0  ...      7        1180              0   \n",
      "1      7242     2.0           0     0  ...      7        2170            400   \n",
      "2     10000     1.0           0     0  ...      6         770              0   \n",
      "3      5000     1.0           0     0  ...      7        1050            910   \n",
      "4      8080     1.0           0     0  ...      8        1680              0   \n",
      "\n",
      "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
      "0      1955             0    98178  47.5112 -122.257           1340   \n",
      "1      1951          1991    98125  47.7210 -122.319           1690   \n",
      "2      1933             0    98028  47.7379 -122.233           2720   \n",
      "3      1965             0    98136  47.5208 -122.393           1360   \n",
      "4      1987             0    98074  47.6168 -122.045           1800   \n",
      "\n",
      "   sqft_lot15  \n",
      "0        5650  \n",
      "1        7639  \n",
      "2        8062  \n",
      "3        5000  \n",
      "4        7503  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "DF shape: (21613, 21)\n",
      "\n",
      "Data types: id                 int64\n",
      "date              object\n",
      "price            float64\n",
      "bedrooms           int64\n",
      "bathrooms        float64\n",
      "sqft_living        int64\n",
      "sqft_lot           int64\n",
      "floors           float64\n",
      "waterfront         int64\n",
      "view               int64\n",
      "condition          int64\n",
      "grade              int64\n",
      "sqft_above         int64\n",
      "sqft_basement      int64\n",
      "yr_built           int64\n",
      "yr_renovated       int64\n",
      "zipcode            int64\n",
      "lat              float64\n",
      "long             float64\n",
      "sqft_living15      int64\n",
      "sqft_lot15         int64\n",
      "dtype: object\n",
      "\n",
      "Describe:                  id         price      bedrooms     bathrooms   sqft_living  \\\n",
      "count  2.161300e+04  2.161300e+04  21613.000000  21613.000000  21613.000000   \n",
      "mean   4.580302e+09  5.400881e+05      3.370842      2.114757   2079.899736   \n",
      "std    2.876566e+09  3.671272e+05      0.930062      0.770163    918.440897   \n",
      "min    1.000102e+06  7.500000e+04      0.000000      0.000000    290.000000   \n",
      "25%    2.123049e+09  3.219500e+05      3.000000      1.750000   1427.000000   \n",
      "50%    3.904930e+09  4.500000e+05      3.000000      2.250000   1910.000000   \n",
      "75%    7.308900e+09  6.450000e+05      4.000000      2.500000   2550.000000   \n",
      "max    9.900000e+09  7.700000e+06     33.000000      8.000000  13540.000000   \n",
      "\n",
      "           sqft_lot        floors    waterfront          view     condition  \\\n",
      "count  2.161300e+04  21613.000000  21613.000000  21613.000000  21613.000000   \n",
      "mean   1.510697e+04      1.494309      0.007542      0.234303      3.409430   \n",
      "std    4.142051e+04      0.539989      0.086517      0.766318      0.650743   \n",
      "min    5.200000e+02      1.000000      0.000000      0.000000      1.000000   \n",
      "25%    5.040000e+03      1.000000      0.000000      0.000000      3.000000   \n",
      "50%    7.618000e+03      1.500000      0.000000      0.000000      3.000000   \n",
      "75%    1.068800e+04      2.000000      0.000000      0.000000      4.000000   \n",
      "max    1.651359e+06      3.500000      1.000000      4.000000      5.000000   \n",
      "\n",
      "              grade    sqft_above  sqft_basement      yr_built  yr_renovated  \\\n",
      "count  21613.000000  21613.000000   21613.000000  21613.000000  21613.000000   \n",
      "mean       7.656873   1788.390691     291.509045   1971.005136     84.402258   \n",
      "std        1.175459    828.090978     442.575043     29.373411    401.679240   \n",
      "min        1.000000    290.000000       0.000000   1900.000000      0.000000   \n",
      "25%        7.000000   1190.000000       0.000000   1951.000000      0.000000   \n",
      "50%        7.000000   1560.000000       0.000000   1975.000000      0.000000   \n",
      "75%        8.000000   2210.000000     560.000000   1997.000000      0.000000   \n",
      "max       13.000000   9410.000000    4820.000000   2015.000000   2015.000000   \n",
      "\n",
      "            zipcode           lat          long  sqft_living15     sqft_lot15  \n",
      "count  21613.000000  21613.000000  21613.000000   21613.000000   21613.000000  \n",
      "mean   98077.939805     47.560053   -122.213896    1986.552492   12768.455652  \n",
      "std       53.505026      0.138564      0.140828     685.391304   27304.179631  \n",
      "min    98001.000000     47.155900   -122.519000     399.000000     651.000000  \n",
      "25%    98033.000000     47.471000   -122.328000    1490.000000    5100.000000  \n",
      "50%    98065.000000     47.571800   -122.230000    1840.000000    7620.000000  \n",
      "75%    98118.000000     47.678000   -122.125000    2360.000000   10083.000000  \n",
      "max    98199.000000     47.777600   -121.315000    6210.000000  871200.000000  \n",
      "\n",
      "isna sum: id               0\n",
      "date             0\n",
      "price            0\n",
      "bedrooms         0\n",
      "bathrooms        0\n",
      "sqft_living      0\n",
      "sqft_lot         0\n",
      "floors           0\n",
      "waterfront       0\n",
      "view             0\n",
      "condition        0\n",
      "grade            0\n",
      "sqft_above       0\n",
      "sqft_basement    0\n",
      "yr_built         0\n",
      "yr_renovated     0\n",
      "zipcode          0\n",
      "lat              0\n",
      "long             0\n",
      "sqft_living15    0\n",
      "sqft_lot15       0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel('./kc_house_data.csv')\n",
    "\n",
    "model.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id             date      price  bedrooms  bathrooms  \\\n",
      "7252  6762700020  20141013T000000  7700000.0         6       8.00   \n",
      "3914  9808700762  20140611T000000  7062500.0         5       4.50   \n",
      "9254  9208900037  20140919T000000  6885000.0         6       7.75   \n",
      "4411  2470100110  20140804T000000  5570000.0         5       5.75   \n",
      "1448  8907500070  20150413T000000  5350000.0         5       5.00   \n",
      "\n",
      "      sqft_living  sqft_lot  floors  waterfront  view  ...  grade  sqft_above  \\\n",
      "7252        12050     27600     2.5           0     3  ...     13        8570   \n",
      "3914        10040     37325     2.0           1     2  ...     11        7680   \n",
      "9254         9890     31374     2.0           0     4  ...     13        8860   \n",
      "4411         9200     35069     2.0           0     0  ...     13        6200   \n",
      "1448         8000     23985     2.0           0     4  ...     12        6720   \n",
      "\n",
      "      sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
      "7252           3480      1910          1987    98102  47.6298 -122.323   \n",
      "3914           2360      1940          2001    98004  47.6500 -122.214   \n",
      "9254           1030      2001             0    98039  47.6305 -122.240   \n",
      "4411           3000      2001             0    98039  47.6289 -122.233   \n",
      "1448           1280      2009             0    98004  47.6232 -122.220   \n",
      "\n",
      "      sqft_living15  sqft_lot15  \n",
      "7252           3940        8800  \n",
      "3914           3930       25449  \n",
      "9254           4540       42730  \n",
      "4411           3560       24345  \n",
      "1448           4600       21750  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "model.set_target('price')\n",
    "\n",
    "model.print_sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           count          mean            std       min        25%       50%  \\\n",
      "bedrooms                                                                       \n",
      "0           13.0  4.095038e+05  358682.627507  139950.0  235000.00  288000.0   \n",
      "1          199.0  3.176429e+05  148864.955017   75000.0  222000.00  299000.0   \n",
      "2         2760.0  4.013727e+05  198051.827269   78000.0  269837.50  374000.0   \n",
      "3         9824.0  4.662321e+05  262469.771863   82000.0  295487.50  413000.0   \n",
      "4         6882.0  6.354195e+05  388594.441911  100000.0  376962.50  549997.5   \n",
      "5         1601.0  7.865998e+05  596204.003693  133000.0  438000.00  620000.0   \n",
      "6          272.0  8.255206e+05  799238.819958  175000.0  435000.00  650000.0   \n",
      "7           38.0  9.511847e+05  739953.558961  280000.0  539250.00  728580.0   \n",
      "8           13.0  1.105077e+06  897495.725295  340000.0  490000.00  700000.0   \n",
      "9            6.0  8.939998e+05  381533.900984  450000.0  624999.25  817000.0   \n",
      "10           3.0  8.193333e+05  284677.595419  650000.0  655000.00  660000.0   \n",
      "11           1.0  5.200000e+05            NaN  520000.0  520000.00  520000.0   \n",
      "33           1.0  6.400000e+05            NaN  640000.0  640000.00  640000.0   \n",
      "\n",
      "                75%        max  \n",
      "bedrooms                        \n",
      "0          355000.0  1295650.0  \n",
      "1          390000.0  1247000.0  \n",
      "2          490000.0  3278000.0  \n",
      "3          560000.0  3800000.0  \n",
      "4          765000.0  4489000.0  \n",
      "5          913888.0  7062500.0  \n",
      "6          896250.0  7700000.0  \n",
      "7          946500.0  3200000.0  \n",
      "8         1650000.0  3300000.0  \n",
      "9         1193500.0  1400000.0  \n",
      "10         904000.0  1148000.0  \n",
      "11         520000.0   520000.0  \n",
      "33         640000.0   640000.0  \n"
     ]
    }
   ],
   "source": [
    "model.find_outliers('bedrooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    0.454541\n",
      "4    0.318419\n",
      "2    0.127701\n",
      "5    0.074076\n",
      "6    0.012585\n",
      "Name: bedrooms, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "model.check_value_proportions(field='bedrooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedrooms\n",
      "8     1.105077e+06\n",
      "7     9.511847e+05\n",
      "9     8.939998e+05\n",
      "6     8.255206e+05\n",
      "10    8.193333e+05\n",
      "5     7.865998e+05\n",
      "33    6.400000e+05\n",
      "4     6.354195e+05\n",
      "11    5.200000e+05\n",
      "3     4.662321e+05\n",
      "0     4.095038e+05\n",
      "2     4.013727e+05\n",
      "1     3.176429e+05\n",
      "Name: price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "model.print_sorted(field='price', groupby='bedrooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to add some methods to our EDACleaning object that actually alter our original dataframe! The new methods are added at the bottom:\n",
    "- drop_dupes\n",
    "- remove_outliers\n",
    "- _calculate_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDACleaning:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_target(self, target: str):\n",
    "        \"\"\"set model target variable\"\"\"\n",
    "        self.target = target\n",
    "\n",
    "    def print_statistics(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Print basic dataframe statistics\"\"\"\n",
    "        print(df.head())\n",
    "        print(f\"DF shape: {df.shape}\\n\")\n",
    "        print(f\"Data types: {df.dtypes}\\n\")\n",
    "        print(f\"Describe: {df.describe()}\\n\")\n",
    "        print(f\"isna sum: {df.isna().sum()}\\n\")\n",
    "\n",
    "    def print_sorted(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        field: str,\n",
    "        groupby: Optional[str],\n",
    "        asc: bool,\n",
    "    ) -> None:\n",
    "        if groupby:\n",
    "            print(df.groupby(groupby)[field].mean().sort_values(ascending=asc))\n",
    "        else:\n",
    "            print(df.sort_values(field, ascending=asc).head())\n",
    "\n",
    "    def check_value_proportions(self, df: pd.DataFrame, field: str) -> None:\n",
    "        print(df[field].value_counts(normalize=True).head())\n",
    "\n",
    "    def find_outliers(self, df: pd.DataFrame, field: str) -> None:\n",
    "        find_outliers = df.groupby(field)[self.target].describe()\n",
    "        outlier_report = find_outliers.sort_values(\"mean\", ascending=False).head(20)\n",
    "        print(outlier_report)\n",
    "\n",
    "    def drop_dupes(self, df: pd.DataFrame, subset: list=None) -> pd.DataFrame:\n",
    "        if subset:\n",
    "            df.drop_duplicates(subset, keep=\"last\", inplace=True)\n",
    "        else:\n",
    "            df.drop_duplicates(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def remove_outliers(\n",
    "        self, df: pd.DataFrame, fields: list, method: str, range: float\n",
    "    ) -> pd.DataFrame:\n",
    "        if method == \"iqr\":\n",
    "            for field in fields:\n",
    "                lower_range, upper_range = self._calculate_iqr(df[field], range)\n",
    "                df = df.drop(\n",
    "                    df[(df[field] > upper_range) | (df[field] < lower_range)].index\n",
    "                )\n",
    "        return df\n",
    "\n",
    "    def _calculate_iqr(self, column: pd.Series, range: float) -> Literal(float, float):\n",
    "        \"\"\"return the lower range and upper range for the data based on IQR\n",
    "        Arguments:\n",
    "        column - column to be evaluated\n",
    "        iqr_level - iqr range to be evaluated\n",
    "        \"\"\"\n",
    "        Q1, Q3 = np.percentile(column, [25, 75])\n",
    "        iqr = Q3 - Q1\n",
    "        lower_range = Q1 - (range * iqr)\n",
    "        upper_range = Q3 + (range * iqr)\n",
    "        return lower_range, upper_range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll might notice that in our type hints, these functions note that they return a pd.DataFrame instead of None. These are our first cleaning functions that actually make a modification to our original dataframe.\n",
    "\n",
    "Now we add the methods that call these to our BaseModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    def __init__(self, filename: str):\n",
    "        self.df = self._load_file(filename)\n",
    "        self.target = None\n",
    "        self.cleaner = EDACleaning()\n",
    "\n",
    "    def _load_file(self, filename: str) -> pd.DataFrame:\n",
    "        \"\"\"Load file from filename and set target field\n",
    "        Args:\n",
    "            filename (str): filename in csv format\n",
    "        Returns:\n",
    "            pd.DataFrame: df loaded from file\n",
    "        \"\"\"\n",
    "        return pd.read_csv(filename, on_bad_lines=\"skip\")\n",
    "\n",
    "    def set_target(self, target: str) -> None:\n",
    "        \"\"\"Sets model target field\n",
    "        Args:\n",
    "            target (str): target: target field for model\n",
    "        \"\"\"\n",
    "        self.target = target\n",
    "        self.cleaner.set_target(target)\n",
    "\n",
    "    def print_statistics(self) -> None:\n",
    "        \"\"\"Print basic statistics for data\"\"\"\n",
    "        self.cleaner.print_statistics(self.df)\n",
    "\n",
    "    def print_sorted(\n",
    "        self,\n",
    "        field: Optional[str] = None,\n",
    "        groupby: Optional[str] = None,\n",
    "        asc: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Prints sorted based on provided field. Will use target if no field provided.\n",
    "        Args:\n",
    "            field (_type_, optional): Will sort by this field. Defaults to target.\n",
    "            asc (bool, optional): Sort ascending. Defaults to False.\n",
    "            groupby (_type_, optional): If entered, will group by this field. Defaults to None.\n",
    "        \"\"\"\n",
    "        if not field:\n",
    "            field = self.target\n",
    "        self.cleaner.print_sorted(df=self.df, field=field, asc=asc, groupby=groupby)\n",
    "\n",
    "    def check_value_proportions(self, field: Optional[str] = None) -> None:\n",
    "        \"\"\"Will print value counts for field\n",
    "        Args:\n",
    "            field (_type_, optional): Will report on this field. Defaults to target.\n",
    "        \"\"\"\n",
    "        if not field:\n",
    "            field = self.target\n",
    "        self.cleaner.check_value_proportions(self.df, field)\n",
    "\n",
    "    def find_outliers(self, field: str) -> None:\n",
    "        self.cleaner.find_outliers(self.df, field)\n",
    "\n",
    "    def drop_dupes(self, subset: Optional[list] = None):\n",
    "        \"\"\"drops duplicate dataframe rows\n",
    "        Args:\n",
    "            subset (list, optional): Subset on which to drop dupes. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.df = self.cleaner.drop_dupes(self.df, subset)\n",
    "\n",
    "    def remove_outliers(\n",
    "        self,\n",
    "        fields: list = [],\n",
    "        method: Optional[str] = \"iqr\",\n",
    "        range: Optional[float] = 1.5,\n",
    "        save: Optional[bool] = True,\n",
    "    ) -> None:\n",
    "        \"\"\"removes outliers, defaultings to IQR with a default IQR range of 1.5\n",
    "        Args:\n",
    "            fields (list): list of fields. Must be list even if one item.\n",
    "            method (str, optional): outlier removal method. Defaults to \"iqr\".\n",
    "            range (float, optional): IQR range. Defaults to 1.5.\n",
    "        \"\"\"\n",
    "        self.df = self.cleaner.remove_outliers(self.df, fields, method, range)\n",
    "\n",
    "    def reset_df_index(self, save: Optional[bool] = True) -> None:\n",
    "        \"\"\"resets dataframe index\"\"\"\n",
    "        print(self.df.head())\n",
    "        self.df.reset_index(inplace=True, drop=True)\n",
    "        print(self.df.head())\n",
    "\n",
    "    @abstractmethod\n",
    "    def split_data(self, stratify: Optional[bool]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def basic_regression(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice a method here that wasn't in the EDACleaning object. reset_df_index is only located here. This method is SO simple that I opted to keep its full implementation here instead of using a pass-through method - but for the sake of consistency, you could opt to pass it through.\n",
    "\n",
    "We could be done here, but now that we've added methods that change our initial data, we've run into a potential problem point. What if we make a change and then don't like our change? As it stands, we would have to go all the way back to the beginning of our work and remake our model object from scratch with our filename, in order to reset the data.\n",
    "\n",
    "Instead of doing that, we're going to implement an undo() method that undoes our last change call. We're going to do the following steps:\n",
    "- Implement a _set_save() method that saves our dataframe state\n",
    "- Implement an undo() method that restores the last saved state\n",
    "- Insert a save() call into any method that makes data frame changes, and default saving to True.\n",
    "\n",
    "Here's what this all looks like, implemented in our BaseModel class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    def __init__(self, filename: str):\n",
    "        self.df = self._load_file(filename)\n",
    "        self.target = None\n",
    "        self.cleaner = EDACleaning()\n",
    "\n",
    "    def _load_file(self, filename: str) -> pd.DataFrame:\n",
    "        \"\"\"Load file from filename and set target field\n",
    "        Args:\n",
    "            filename (str): filename in csv format\n",
    "        Returns:\n",
    "            pd.DataFrame: df loaded from file\n",
    "        \"\"\"\n",
    "        return pd.read_csv(filename, on_bad_lines=\"skip\")\n",
    "\n",
    "    def set_target(self, target: str) -> None:\n",
    "        \"\"\"Sets model target field\n",
    "        Args:\n",
    "            target (str): target: target field for model\n",
    "        \"\"\"\n",
    "        self.target = target\n",
    "        self.cleaner.set_target(target)\n",
    "\n",
    "    def _set_save(self, saved: str) -> None:\n",
    "        \"\"\"Sets a save point on the dataframe before performing an alteration task\n",
    "\n",
    "        Args:\n",
    "            saved (str): description of saved task to report if undone\n",
    "        \"\"\"\n",
    "        self.saved_df = self.df.copy()\n",
    "        self.saved_action = saved\n",
    "\n",
    "    def undo(self) -> None:\n",
    "        \"\"\"Undoes the last data frame alteration task, and reports on Undo\"\"\"\n",
    "        self.df = self.saved_df()\n",
    "        print(f\"Undid last change: {self.saved_action}\")\n",
    "\n",
    "    def print_statistics(self) -> None:\n",
    "        \"\"\"Print basic statistics for data\"\"\"\n",
    "        self.cleaner.print_statistics(self.df)\n",
    "\n",
    "    def print_sorted(\n",
    "        self,\n",
    "        field: Optional[str] = None,\n",
    "        groupby: Optional[str] = None,\n",
    "        asc: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Prints sorted based on provided field. Will use target if no field provided.\n",
    "        Args:\n",
    "            field (_type_, optional): Will sort by this field. Defaults to target.\n",
    "            asc (bool, optional): Sort ascending. Defaults to False.\n",
    "            groupby (_type_, optional): If entered, will group by this field. Defaults to None.\n",
    "        \"\"\"\n",
    "        if not field:\n",
    "            field = self.target\n",
    "        self.cleaner.print_sorted(df=self.df, field=field, asc=asc, groupby=groupby)\n",
    "\n",
    "    def check_value_proportions(self, field: Optional[str] = None) -> None:\n",
    "        \"\"\"Will print value counts for field\n",
    "        Args:\n",
    "            field (_type_, optional): Will report on this field. Defaults to target.\n",
    "        \"\"\"\n",
    "        if not field:\n",
    "            field = self.target\n",
    "        self.cleaner.check_value_proportions(self.df, field)\n",
    "\n",
    "    def find_outliers(self, field: str) -> None:\n",
    "        self.cleaner.find_outliers(self.df, field)\n",
    "\n",
    "    def drop_dupes(self, subset: Optional[list] = None, save: Optional[bool] = True):\n",
    "        \"\"\"Save point, then drops duplicate dataframe rows\n",
    "        Args:\n",
    "            subset (list, optional): Subset on which to drop dupes. Defaults to None.\n",
    "            save (boolean, optional): Toggles to save. Defaults to None.\n",
    "        \"\"\"\n",
    "        if save:\n",
    "            self._set_save(\"drop_dupes\")\n",
    "        self.df = self.cleaner.drop_dupes(self.df, subset)\n",
    "\n",
    "    def remove_outliers(\n",
    "        self,\n",
    "        fields: list = [],\n",
    "        method: Optional[str] = \"iqr\",\n",
    "        range: Optional[float] = 1.5,\n",
    "        save: Optional[bool] = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Save point, then removes outliers, defaultings to IQR with a default IQR range of 1.5\n",
    "        Args:\n",
    "            fields (list): list of fields. Must be list even if one item.\n",
    "            method (str, optional): outlier removal method. Defaults to \"iqr\".\n",
    "            range (float, optional): IQR range. Defaults to 1.5.\n",
    "            save (boolean, optional): Toggles to save. Defaults to None.\n",
    "        \"\"\"\n",
    "        if save:\n",
    "            self._set_save(\"remove_outliers\")\n",
    "        self.df = self.cleaner.remove_outliers(self.df, fields, method, range)\n",
    "\n",
    "    def reset_index(self, save: Optional[bool] = True) -> None:\n",
    "        \"\"\"Save point, then resets dataframe index\"\"\"\n",
    "        print(self.df.head())\n",
    "        if save:\n",
    "            self._set_save(\"reset_index\")\n",
    "        self.df.reset_index(inplace=True, drop=True)\n",
    "        print(self.df.head())\n",
    "\n",
    "    @abstractmethod\n",
    "    def split_data(self, stratify: Optional[bool]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def basic_regression(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our _set_save is an internal-only method that makes a copy of our current data frame, as well as records the name of the last taken action.\n",
    "Our undo() call will return the data frame to the last save point, and report about what was rolled back.\n",
    "\n",
    "_set_save() is implemented in all methods where the data frame is altered, so long as save=True, which is the default behavior. A message is sent to the _set_save() call to be recorded as the rollback message.\n",
    "\n",
    "It is now very easy for us to roll back our last action if we realize we made a mistake, without resetting our entire object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, we implemented new cleaning methods, and set up the simplest possible interfaces for us to access these methods. For our method which alter our data, we set up a save system that allows us an undo. Finally, we make use of default parameters for our functions, to allow us to access them as easily as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('object_oriented_machine_learning-cyZdX5gt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c92e476e37dcf0c11c9fec65999947cee5b5777dc27b66638545f02a2906077b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
